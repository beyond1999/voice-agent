```
# asr
root@Eric:/mnt/d/github-repo/ai-role-chat# source .venv-asr/bin/activate

(.venv-asr) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/asr#

(.venv-asr) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/asr# python3 asr_app.py 
```

```
#qwen
root@Eric:/mnt/d/github-repo/ai-role-chat# source .venv-qwen/bin/activate
(.venv-qwen) root@Eric:/mnt/d/github-repo/ai-role-chat# cd backend/llm/
(.venv-qwen) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/llm# python3 llm_app.py
```



æŠŠ **Qwen2.5 çš„ GGUF** ç”¨ **llama.cpp/`llama-server` è·‘åœ¨ 8080**ï¼Œå†ç”¨ **FastAPI ä»£ç†è·‘åœ¨ 8001** ï¼Œå‰ç«¯/å…¶ä»–å®¢æˆ·ç«¯éƒ½æ˜¯æ‰“ **8001**ï¼Œä»£ç†å†è½¬åˆ° 8080ã€‚å…¸å‹å¯åŠ¨æ–¹å¼å°±æ˜¯ä¸‹é¢è¿™ä¸¤æ¡ã€‚

### ä¸¤æ¡å¯åŠ¨å‘½ä»¤

**1) å¯åŠ¨ llama.cpp çš„æœåŠ¡å™¨ï¼ˆQwen æ¨¡å‹åœ¨ 8080ï¼‰**

```bash
MODEL=~/models/qwen2.5/3b-instruct/Qwen2.5-3B-Instruct-Q4_K_M.gguf
cd ~/work/llama.cpp/build/bin
./llama-server -m "$MODEL" --host 0.0.0.0 --port 8080 \
  -c 4096 --temp 0.7 --repeat-penalty 1.1 --n-gpu-layers 35
```

**2) å¯åŠ¨ LLM ä»£ç†ï¼ˆFastAPI åœ¨ 8001ï¼Œè½¬å‘åˆ° 8080ï¼‰**

```bash
export LLAMA_BASE=http://127.0.0.1:8080
python -m uvicorn backend.llm.llm_app:app --host 127.0.0.1 --port 8001 --workers 1 --reload false
```

å‰ç«¯ï¼ˆæˆ–ä½ çš„è°ƒè¯•è„šæœ¬ï¼‰ä¸€èˆ¬æ˜¯æ‰“ï¼š

- `POST http://127.0.0.1:8001/llm`
   å¥åº·æ£€æŸ¥å¸¸ç”¨ï¼š
- `curl http://127.0.0.1:8001/health`ï¼ˆä½ å†™è¿‡è¿™ä¸ªç«¯ç‚¹ï¼‰
- `curl http://127.0.0.1:8080/v1/models`ï¼ˆllama.cpp çš„ OpenAI å…¼å®¹æ¥å£ï¼‰

------

## ç°åœ¨å¦‚ä½•å¿«é€Ÿâ€œå®é”¤â€å½“æ—¶çš„ç«¯å£ä¸å‘½ä»¤

### A. çœ‹è¿›ç¨‹/ç›‘å¬ç«¯å£

```bash
ps aux | egrep 'llama-server|uvicorn'
lsof -iTCP:8001 -sTCP:LISTEN
lsof -iTCP:8080 -sTCP:LISTEN
```

### B. æ‰“æ¥å£éªŒè¯

```bash
curl -s http://127.0.0.1:8001/health
curl -s http://127.0.0.1:8080/v1/models | jq .
```

### C. æŸ¥å†å²å‘½ä»¤ï¼ˆbash/zshï¼‰

**bashï¼š**

```bash
export HISTTIMEFORMAT='%F %T  '
history | grep -n -E 'llama-server|uvicorn|Qwen|llama.cpp'
```

**zshï¼š**

```bash
grep -n -E 'llama-server|uvicorn|Qwen|llama.cpp' ~/.zsh_history
# zsh è¡Œé¦–ä¼šæœ‰ :<epoch>:<flags>; ä½ å¯ç²—ç•¥ç”¨ grep å…ˆæ‰¾å‘½ä»¤ï¼Œå†å¯¹ç…§ 9/28 å½“å¤©çš„å…¶ä»–ä¸Šä¸‹æ–‡
```

> å¦‚æœä½ é‚£å¤©æ˜¯ç”¨ **tmux/screen** è·‘çš„ï¼š

```bash
tmux ls
tmux capture-pane -p -S -1000  # åœ¨ä¼šè¯é‡Œå›æ»šçœ‹å¯åŠ¨å‘½ä»¤
```

> å¦‚æœæ˜¯ **Docker** è·‘çš„ï¼ˆå¶å°”ä¹Ÿæœ‰äººè¿™ä¹ˆåšï¼‰ï¼š

```bash
docker ps --format 'table {{.Names}}\t{{.Ports}}' | grep -E '8001|8080'
docker logs <å®¹å™¨å> | tail -n 200
```

------

## è®°å¿†è¦ç‚¹ï¼ˆæ–¹ä¾¿ä½  PPT æˆ–æ–‡æ¡£æ ‡æ³¨ï¼‰

- **8080 = llama-server (Qwen2.5 GGUF æœ¬åœ°æ¨ç†)**
- **8001 = ä½ çš„ FastAPI LLM ä»£ç†ï¼ˆOpenAI å…¼å®¹ä¼˜å…ˆï¼Œ/completion å›é€€ï¼‰**
- å‰ç«¯/è„šæœ¬ç»Ÿä¸€æ‰“ 8001ï¼Œä»£ç†å†è½¬åˆ° 8080

è¦ä¸è¦æˆ‘é¡ºæ‰‹æŠŠè¿™æ®µâ€œæ’éšœ/è‡ªè¯ç«¯å£å½’å±â€çš„å°æ¸…å•ï¼Œæ’åˆ°ä½  PPT çš„â€œé™„å½•/æ¼”ç¤ºç¯å¢ƒâ€é¡µé‡Œï¼Ÿ

---

éŸ³é¢‘æµè¾“å…¥ï¼ˆPCMï¼‰
    â†“
ç¯å½¢ç¼“å†²åŒºï¼ˆä¿å­˜æœ€è¿‘å‡ ç§’éŸ³é¢‘ï¼‰
    â†“
rms_recent() â†’ è®¡ç®—èƒ½é‡
    â†“
åˆ¤æ–­ is_silence = (rms < é˜ˆå€¼)
    â†“
Endpointor.update(now_ms, partial_text, is_silence, tick_ms)





éŸ³é¢‘æµè¾“å…¥ï¼ˆPCMï¼‰
    â†“
ç¯å½¢ç¼“å†²åŒºï¼ˆä¿å­˜æœ€è¿‘å‡ ç§’éŸ³é¢‘ï¼‰
    â†“
rms_recent() â†’ è®¡ç®—èƒ½é‡
    â†“
åˆ¤æ–­ is_silence = (rms < é˜ˆå€¼)
    â†“
Endpointor.update(now_ms, partial_text, is_silence, tick_ms)

| æ¨¡å‹                  | é‡åŒ–ç­‰çº§  | çº¦æ˜¾å­˜å ç”¨            | å¯åŠ¨åé€Ÿåº¦ |
| --------------------- | --------- | --------------------- | ---------- |
| Qwen2.5-7B-Q8_0       | 8-bit     | â‰ˆ 13 GB â†’ âŒæ”¾ä¸ä¸‹     |            |
| Qwen2.5-3B-Q8_0       | 8-bit     | â‰ˆ 7 GB â†’ ğŸŸ¡ å‹‰å¼º       |            |
| **Qwen2.5-3B-Q4_K_M** | **4-bit** | **â‰ˆ 3.5â€“4 GB âœ…é€‚é…**  |            |
| Qwen1.5-1.8B-Q4_K_M   | 4-bit     | â‰ˆ 2 GB â†’ ğŸŸ¢ ä½†ç²¾åº¦ç•¥ä½ |            |