```
# asr
root@Eric:/mnt/d/github-repo/ai-role-chat# source .venv-asr/bin/activate

(.venv-asr) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/asr#

(.venv-asr) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/asr# python3 asr_app.py 
```

```
#qwen
root@Eric:/mnt/d/github-repo/ai-role-chat# source .venv-qwen/bin/activate
(.venv-qwen) root@Eric:/mnt/d/github-repo/ai-role-chat# cd backend/llm/
(.venv-qwen) root@Eric:/mnt/d/github-repo/ai-role-chat/backend/llm# python3 llm_app.py
```



把 **Qwen2.5 的 GGUF** 用 **llama.cpp/`llama-server` 跑在 8080**，再用 **FastAPI 代理跑在 8001** ，前端/其他客户端都是打 **8001**，代理再转到 8080。典型启动方式就是下面这两条。

### 两条启动命令

**1) 启动 llama.cpp 的服务器（Qwen 模型在 8080）**

```bash
MODEL=~/models/qwen2.5/3b-instruct/Qwen2.5-3B-Instruct-Q4_K_M.gguf
cd ~/work/llama.cpp/build/bin
./llama-server -m "$MODEL" --host 0.0.0.0 --port 8080 \
  -c 4096 --temp 0.7 --repeat-penalty 1.1 --n-gpu-layers 35
```

**2) 启动 LLM 代理（FastAPI 在 8001，转发到 8080）**

```bash
export LLAMA_BASE=http://127.0.0.1:8080
python -m uvicorn backend.llm.llm_app:app --host 127.0.0.1 --port 8001 --workers 1 --reload false
```

前端（或你的调试脚本）一般是打：

- `POST http://127.0.0.1:8001/llm`
   健康检查常用：
- `curl http://127.0.0.1:8001/health`（你写过这个端点）
- `curl http://127.0.0.1:8080/v1/models`（llama.cpp 的 OpenAI 兼容接口）

------

## 现在如何快速“实锤”当时的端口与命令

### A. 看进程/监听端口

```bash
ps aux | egrep 'llama-server|uvicorn'
lsof -iTCP:8001 -sTCP:LISTEN
lsof -iTCP:8080 -sTCP:LISTEN
```

### B. 打接口验证

```bash
curl -s http://127.0.0.1:8001/health
curl -s http://127.0.0.1:8080/v1/models | jq .
```

### C. 查历史命令（bash/zsh）

**bash：**

```bash
export HISTTIMEFORMAT='%F %T  '
history | grep -n -E 'llama-server|uvicorn|Qwen|llama.cpp'
```

**zsh：**

```bash
grep -n -E 'llama-server|uvicorn|Qwen|llama.cpp' ~/.zsh_history
# zsh 行首会有 :<epoch>:<flags>; 你可粗略用 grep 先找命令，再对照 9/28 当天的其他上下文
```

> 如果你那天是用 **tmux/screen** 跑的：

```bash
tmux ls
tmux capture-pane -p -S -1000  # 在会话里回滚看启动命令
```

> 如果是 **Docker** 跑的（偶尔也有人这么做）：

```bash
docker ps --format 'table {{.Names}}\t{{.Ports}}' | grep -E '8001|8080'
docker logs <容器名> | tail -n 200
```

------

## 记忆要点（方便你 PPT 或文档标注）

- **8080 = llama-server (Qwen2.5 GGUF 本地推理)**
- **8001 = 你的 FastAPI LLM 代理（OpenAI 兼容优先，/completion 回退）**
- 前端/脚本统一打 8001，代理再转到 8080

要不要我顺手把这段“排障/自证端口归属”的小清单，插到你 PPT 的“附录/演示环境”页里？

---

音频流输入（PCM）
    ↓
环形缓冲区（保存最近几秒音频）
    ↓
rms_recent() → 计算能量
    ↓
判断 is_silence = (rms < 阈值)
    ↓
Endpointor.update(now_ms, partial_text, is_silence, tick_ms)





音频流输入（PCM）
    ↓
环形缓冲区（保存最近几秒音频）
    ↓
rms_recent() → 计算能量
    ↓
判断 is_silence = (rms < 阈值)
    ↓
Endpointor.update(now_ms, partial_text, is_silence, tick_ms)

| 模型                  | 量化等级  | 约显存占用            | 启动后速度 |
| --------------------- | --------- | --------------------- | ---------- |
| Qwen2.5-7B-Q8_0       | 8-bit     | ≈ 13 GB → ❌放不下     |            |
| Qwen2.5-3B-Q8_0       | 8-bit     | ≈ 7 GB → 🟡 勉强       |            |
| **Qwen2.5-3B-Q4_K_M** | **4-bit** | **≈ 3.5–4 GB ✅适配**  |            |
| Qwen1.5-1.8B-Q4_K_M   | 4-bit     | ≈ 2 GB → 🟢 但精度略低 |            |